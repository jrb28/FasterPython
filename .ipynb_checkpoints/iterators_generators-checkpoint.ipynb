{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterables, Iterators, and Generators   <a name='ItItGen' />\n",
    "\n",
    "## _Iterables_   <a name='Iterables' />\n",
    "\n",
    "We are all now very familiar with <code>for</code> loops, which provide a basis for an effective functional definition of an _iterable_ data type, which is a data type that:\n",
    "\n",
    "- can contain 0, 1, or many elements\n",
    "- can be used with a <code>for</code> statement so that each element of the _iterable_ data type is made available in the iterations of the <code>for</code> loop\n",
    "\n",
    "<code>for x in _put-iterable-here_:\n",
    "    ...</code>\n",
    "    \n",
    "These are well-known basic Python _iterable_ data types:\n",
    "\n",
    "- list\n",
    "- range()\n",
    "- tuple\n",
    "- dictionary\n",
    "- set\n",
    "- string\n",
    "\n",
    "The term _iterable_ is often used as a noun, as a short form of '_iterable_ data type'.\n",
    "\n",
    "The cells below illustrate how these data types are iterable in <code>for</code> loops.\n",
    "\n",
    "We will later show other data types that are iterable and, therefore, can be included in <code>for</code> loops and in list comprehension statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [0, 1, 2]\n",
    "my_range = range(3)\n",
    "my_tup = (0, 1, 2)\n",
    "my_dct = {0:'zero', 1:'one', 2:'two'}\n",
    "my_set = {0, 1, 2}\n",
    "my_str = 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in my_list:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in my_range:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in my_tup:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in my_dct:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in my_dct.items():\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in my_set:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in my_str:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers are powerful because they can iterate through large data sets and make computations.  Iterable data and loops are the programming components that enable that capability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterators  <a name='Iterators' />\n",
    "\n",
    "Iterators are like iterables in that a <code>for</code> loop can be used to present their elements one at a time, but they are different from iterables in these ways:\n",
    "- Only a single pass is permitted through the iterator, in which each of the elements is retrieved once in sequence.\n",
    "  - Once exhausted, no further elements can be retrieved from an iterator without re-creating it.\n",
    "  - Once exhausted, the iterator returns an <code>StopIteration</code> exception, which is gracefully handled (you won't see evidence of it) when iterators are used in <code>for</code> loops.\n",
    "- The next iterator element can be retrieved using the <code>next()</code> function.\n",
    "\n",
    "The built-in <code>iter()</code> function creates an iterator from a data type that is a collection of elements, as shown below.  \n",
    "\n",
    "Iterators can be faster and use less memory than storing an entire list in memory, for example, when data sets are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(it))\n",
    "print(next(it))\n",
    "print(next(it))\n",
    "print(next(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below, if executed immediately after the cell above, will return nothing because the iterator is exhausted.  The iterator needs to be reset being using it again.\n",
    "\n",
    "Notice that rather than seeing evidence of an exception/error, instead nothing happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in it:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below will return something because the iterator is reset in the first statement. In addition, you will notice that the <code>for</code> loop does not show the <code>StopIteration</code> error but, instead, just uses it as an indication that the loop shuld be terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(my_list)\n",
    "for i in it:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(range(3))\n",
    "for i in it:\n",
    "    print(i)\n",
    "    \n",
    "# This next statement will cause an error\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using the <code>iter()</code> on an iterable does not necessarily provide any advantage in speed and memory usage because, as in the case above, the list already exists in memory.  But, these examples are useful to demonstrate how an iterator functions,  Iterators do provide speed and memory advantages in other circumstances.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below give a quick example of how iterators can reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "\n",
    "with open('files/numbers.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "for i in range(len(data)):\n",
    "    data[i] = int(data[i].strip())\n",
    "    \n",
    "answer = sum(data)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "\n",
    "answer = 0\n",
    "with open('files/numbers.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        answer += int(line.strip())\n",
    "    \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators  <a name='Generators' />\n",
    "\n",
    "Generators are a specific type of iterator.  They have all the characteristics of iterators noted above and, in addition, they create a stream of elements with a _function_ rather than merely regurgitating the elements from an existing data structure.\n",
    "\n",
    "Generators can be defined in two ways:\n",
    "- With a function\n",
    "- With a comprehension statement\n",
    "\n",
    "A generator function is just like a regular custom function except that it uses <code>yield</code> rather than <code>return</code> to send values back to the calling statement.  <code>yield</code> return a value or values from one iteration of the function, and then causes the function to pause until it is called next.  When called again, the function will continue by executing until its next <code>yield</code> statement.\n",
    "\n",
    "Generators, define using comprehension, are defined within a set of parentheses.\n",
    "\n",
    "Our first example will be another way to read in numerical data from a text file shown in two way, one using a function and another using comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "\n",
    "def read_convert(filepath):\n",
    "    f =  open(filepath, 'r')\n",
    "    for line in f:\n",
    "        yield int(line.strip())\n",
    "\n",
    "answer = 0\n",
    "for x in read_convert('files/numbers.txt'):\n",
    "    answer += x\n",
    "    \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "\n",
    "read_convert_comp = (int(line.strip()) if line.strip() != '' else 0 for line in open('files/numbers.txt', 'r'))\n",
    "\n",
    "answer = 0\n",
    "for x in read_convert('files/numbers.txt'):\n",
    "    answer += x\n",
    "    \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next example, which generates the Fibonacci sequence one term at a time, is a little more complex.  If $f_0 = 1$ and $f_1 = 1$  are the first two terms in the Fibonacci sequence, then the remaining terms are defined by:\n",
    "\n",
    "$f_i = f_{i-1} + f_{i-2}$,\n",
    "\n",
    "so that the Fibonacci sequence starts with\n",
    "\n",
    "$f = 1,1,2,3,5,8,13,21,34, ...$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute n terms of the Fibonacci sequence '''\n",
    "def fib(n):\n",
    "    t1, t2 = 1, 1\n",
    "    counter = 1\n",
    "    yield t1\n",
    "    \n",
    "    while counter <= n:\n",
    "        counter += 1\n",
    "        t1, t2 = t2, t1 + t2\n",
    "        yield t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that two assignments can be executed on the same line, as is done twice in the code above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in fib(7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>yield</code> statement automatically causes the <code>StopIteration</code> exception once the function terminates, as demonstrated in the next cell.\n",
    "\n",
    "List comprehension and <code>for</code> loops handle the <code>StopIteration</code> exception gracefully.  That is, they simply stop when the exception is encountered without alerting you to it.  That does not happen with <code>while</code> loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = fib(7)\n",
    "while True:\n",
    "    print(next(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators may be infinite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute an infinite number of terms of the Fibonacci sequence '''\n",
    "def fib_1():\n",
    "    t1, t2 = 1, 1\n",
    "    counter = 1\n",
    "    yield t1\n",
    "    \n",
    "    while True:\n",
    "        counter += 1\n",
    "        t1, t2 = t2, t1 + t2\n",
    "        yield t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = fib_1()\n",
    "[next(f) for _ in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The generator is still active '''\n",
    "next(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators can provide an enormous advantage in speed and memory usage because all the elements they return do not need to be stored in memory simultaneously because they are generated on demand, one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This image from a post on medium.com summarizes the relationship between iterables, iterators, generators, and Python data types that are collections of elements (containers).\n",
    "\n",
    "![Summary](images/0_IBiuORdbv0CTKuia.png)\n",
    "\n",
    "https://tonylixu.medium.com/python-yield-iterator-and-generator-introduction-15be182f6135, retrieved 11/22/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text File Input with Base Python Generators <a name='text_w_gen' /> \n",
    "\n",
    "Why read files with a generator?\n",
    "\n",
    "__Large files can be read faster by generators while using less memory for holding the entire file contents at once as is done with base Python statements__\n",
    "\n",
    "We will demonstrate the speed advantage in the subsequent code cells using a longer version of the Norfolk weather file, which is 1M lines long: <code>files/NorfolkWeatherLong.csv</code>.  Each line has an integer day index and a floating point temperature. \n",
    "\n",
    "<code>%%timeit</code> is a function that is built in to jupyter.  It times the execution of the entire cell into which it is placed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Base Python Method\n",
    "\n",
    "... which takes approximately 1 second to read a large code file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "\n",
    "def ssc_2d(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        data = [line.strip().split(',') for line in f.readlines()]\n",
    "        data = [[int(point[0]), float(point[1])] for point in data]\n",
    "        return data\n",
    "\n",
    "filename = 'files/NorfolkWeatherLong.csv'\n",
    "nw1999 = ssc_2d(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large File Input with Base Python Generators\n",
    "\n",
    "The code in the cell below returns a generator that provides access to the text file data.\n",
    "\n",
    "It is almost 10X faster than using a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "from itertools import islice\n",
    "\n",
    "def ssc_2d(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        data = (line.strip().split(',') for line in f.readlines())\n",
    "        data = ((int(point[0]), float(point[1])) for point in data)\n",
    "    return data\n",
    "\n",
    "x = ssc_2d('files/NorfolkWeatherLong.csv')\n",
    "# The data is now available in the generator x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still haven't demonstrated how to use generators to minimize memory usage, but we will do that shortly.\n",
    "\n",
    "Also, these execution times are only for the reading of the data.  We must also consider the ultimate task for which we are inputting the data and the total effect on execution time of using generators.  It is possible to use methods for subsequent plotting or computation that squander the speed gain demonstrated above.    Nonetheless, these quick demonstrations hint at the power and value of generators.\n",
    "\n",
    "Let's consider one use case where we want to plot the first 365 lines of <code>NorfolkWeatherLong.csv</code> (the first year), and let's compare the speed of base Python versus generators.  As previously mentioned, <code>matplotlib</code> will not plot generators, so we need to present it was a list (<code>numpy</code> arrays and <code>pandas</code> series work as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "def ssc_2d(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].strip().split(',')\n",
    "        x.append(int(data[i][0]))\n",
    "        y.append(float(data[i][1]))\n",
    "    return x, y\n",
    "\n",
    "with open('files/NorfolkWeatherLong.csv','r') as f:\n",
    "    nw1999 = f.readlines()\n",
    "x, y = ssc_2d(nw1999)\n",
    "\n",
    "plt.plot(x[:365],y[:365])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def ssc_2d(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        data = (line.strip().split(',') for line in f.readlines())\n",
    "        data = ((int(point[0]), float(point[1])) for point in data)\n",
    "    return data\n",
    "\n",
    "x = ssc_2d('files/NorfolkWeatherLong.csv')\n",
    "\n",
    "plt.plot(*zip(*islice(x, 0, 365)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators provide access text file data and plot it __6X__ faster!\n",
    "\n",
    "What do <code>\\*zip()</code> and <code>\\*islice()</code> do?\n",
    "- <code>*zip()</code> separates the $x$ and $y$ data, which the <code>ssc_2d()</code> returns as tuples for each data point.\n",
    "- <code>islice()</code> is the slice operator for generators akin the the <code>my_list[start:stop:step]</code> statement for lists.  Here, <code>islice()</code> selects the first 365 data points.  This is much faster than transforming the entire generator to a list and then slicing.  The accompanying <code>*</code> removes the outer container of the data so <code>zip()</code> can reformat the 'loose' sublists of two elements each.\n",
    "\n",
    "Multiple graphs are displayed because <code>%%timeit</code> executes the code multiple times as it profiles its execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def ssc_2d(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        data = (line.strip().split(',') for line in f.readlines())\n",
    "        data = ((int(point[0]), float(point[1])) for point in data)\n",
    "    return data\n",
    "\n",
    "x = ssc_2d('files/NorfolkWeatherLong.csv')\n",
    "\n",
    "print([*islice(x, 0, 25)], '\\n\\n')\n",
    "print([*zip(*islice(x, 0, 25))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computations on Large Text Files with Generators in One Pass <a name='comp_w_gen' />  \n",
    "\n",
    "Generators provide a subtantial speed advantage and, actually, a feasibility advantage over base Python methods with extremely large data files are concerned.  This is particularly the case when an entire data set cannot be held in memory.  Generators, in this context, can quickly read the data in chunks, thus avoiding the problem of overloading computer memory if all the data were to be loaded at once.  The complication that sometimes arises is that computation methods need to be designed that work with reading the data in chunks and, if a one-pass comptuation is desired, a way to avoid having summary statistics determined by the entire data set, such as the mean.\n",
    "\n",
    "The first two examples below can be accomplished easily in one pass.  The third example requires thoughtful planning on how to complete the computation in one pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average Computation<a name='mv_avg' />  \n",
    "\n",
    "To compute moving averages with streamed data, we need to amend the approach above where only one value was read at a time.  With an $n$-period moving average, we need to keep track of $n$ numbers at a time while maintaining a moving window with those values in an efficient manner.  We need to consider what are the best data types to:\n",
    "- Quickly update the moving window data as we stream the data\n",
    "- Quickly compute the mean\n",
    "\n",
    "There is not necessarily one data type that will be best for both, so we will need to investigate.  Two possibilities are:\n",
    "- <code>numpy</code> arrays\n",
    "  - These have built-in methods for computing means, which are fast\n",
    "- Python lists\n",
    "  - Might be fast for this application if used properly (avoiding <code>append</code>, but need to compute mean using <code>sum()</code> function, which might be slower then <code>numpy</code>.\n",
    "  - In the code below, a scheme is used where the oldest data element in an $n$-element list is overwritten with the newest element, before taking the average.\n",
    "  \n",
    "Let's use a larger set of 1999 Norfolk weather data: <code>files/NorfolkWeatherLong.csv</code>.\n",
    "\n",
    "The three code sets below represent these approaches:\n",
    "\n",
    "- Using basic Python without list comprehension\n",
    "- Using basic Python with list comprehension\n",
    "- Using <code>numpy</code> \n",
    "- Using a Python generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssc_2d_map(file, convert):\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].strip().split(',')\n",
    "        for j, mk_type in convert.items():\n",
    "            data[i][j] = mk_type(data[i][j])\n",
    "    return [x[1] for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Python list and a <code>for</code> loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "period = 10\n",
    "filepath = 'files/NorfolkWeatherLong.csv'\n",
    "convert_map = {0:int, 1:float}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data = ssc_2d_map(filepath, convert_map)\n",
    "ma = []  # Initialize moving average\n",
    "\n",
    "for i in range(0, len(data) - period + 1):\n",
    "    ma.append(sum(data[i:i + period])/period)\n",
    "\n",
    "plt.plot(ma[:365])\n",
    "plt.show()\n",
    "\n",
    "print(f'Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "period = 10\n",
    "filepath = 'files/NorfolkWeatherLong.csv'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data = ssc_2d_map(filepath, convert_map)\n",
    "ma = [sum(ma[i:i + period])/period for i in range(0,len(ma) - period + 1)]\n",
    "\n",
    "plt.plot(ma[:365])\n",
    "plt.show()\n",
    "\n",
    "print(f'Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Python generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_a_line(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            yield float(line.strip().split(',')[1])\n",
    "\n",
    "start = time.time()\n",
    "window = 10\n",
    "filepath = 'files/NorfolkWeatherLong.csv'\n",
    "read_temp = read_a_line(filepath)\n",
    "mv_avg = []\n",
    "\n",
    "''' Create variable for moving window '''\n",
    "mv_win = [0 for i in range(window)]\n",
    "\n",
    "''' Fill window with initial data '''\n",
    "for i in range(window - 1):\n",
    "    mv_win[i] = next(read_temp)\n",
    "    \n",
    "for temp in read_temp:\n",
    "    i += 1\n",
    "    mv_win[i%window] = temp\n",
    "    mv_avg.append(sum(mv_win)/window)\n",
    "\n",
    "plt.plot(mv_avg[:365])\n",
    "plt.show()\n",
    "\n",
    "print(f'Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downsides and weaknesses of generators are due to entire file contents not being held in memory at any one time:\n",
    "- Any processing/computations of the data must be done as the data is being streamed\n",
    "- Any portion of the input data from a generator that is plotted in <code>matplotlib</code> must be converted to a list or similar data structure.  <code>matplotlib</code> cannot plot generators.\n",
    "\n",
    "The code above reads one line at a time, although it is possible to improve generator performance the best performing generators read files in larger _chunks_.\n",
    "\n",
    "We could optimize inputting data from a text file using the Python <code>open()</code> statement and using an optional argument for the best _chunk size_ (number of lines) to read in each iteration.  But, the <code>pandas</code> package permits us to read files in _chunks_ and permits us to create a generator to do so, as we demonstrate in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Taxi Data <a name='nyc' />  \n",
    "\n",
    "Let's start investigating generators with a somewhat large data set on New York City Taxis.\n",
    "\n",
    "[NYC Taxi Cab Data](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data)\n",
    "\n",
    "This data is about 5.5GB, so it is unlikely to crash your computer, but loading it all into RAM, or attempting to, could be very slow and cause your computer to continually swap data between the hard drive and memory, which is very slow.  \n",
    "\n",
    "We will investigate several methods for loading the data and computing a frequency histogram for the number of passengers while paying attention to speed and memory consumption.\n",
    "- Basic Python: reading all of the data at once.\n",
    "- Basic Python with a generator\n",
    "- Basic Python with a generator function\n",
    "- <code>pandas</code>\n",
    "\n",
    "Please note these methods in the cells that follow:\n",
    "- The 'f' string used to format the output.\n",
    "- The <code>dict.get()</code> statement, which allows a default value to be assigned to a new key if it doesn't already exist in the dictionary.  The effect of <code>fh.get(num_pass, 0)</code> is that is creates a new dictionary element with a key of <code>num_pass</code> if that key doesn't exist and assigns a corrseponding value of zero.  If the key exists, then this statement simply gets the value associates with the key <code>num_pass</code>.\n",
    "- The so-called walrus operator (:=), which permits an assignment statement within a loop declarion or conditional statement\n",
    "- The generator also strips whitespace from the ends of the line of text, including <code>\\n</code>, while splitting it at the commas. \n",
    "- The generator conveniently takes a filename and opens the file within the function rather than needs a filestream passed to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This file is too large to include in a gitHub repo and also too large for everybody in class to download at the same time because it would take forever or we'd crash the local network.  So, this will be a demonstration rather than you runnign the code as well.__ If you do want to test this data later, you can first download the file using the code in the next cell, which take take a while to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "source_path = r\"\\\\files.campus.wm.edu\\jrbrad\\public_html\\data\"\n",
    "dest_path = r\"files\"\n",
    "filename = '\\\\train.csv'\n",
    "\n",
    "shutil.copy(source_path + filename, dest_path + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Python reading the entire file at one time to create frequency histogram data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "file_name = 'files/train.csv'\n",
    "fh = {}\n",
    "col_idx = 7\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    heading = f.readline()[col_idx]  # Reads header line\n",
    "    data = f.readlines()\n",
    "\n",
    "for pt in data:\n",
    "    pt = pt.strip().split(',')\n",
    "    pt = int(pt[col_idx])\n",
    "    fh[pt] = fh.get(pt, 0) + 1\n",
    "        \n",
    "print(heading, fh)\n",
    "print(f'Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A filestream created with the <code>open()</code> statement is actually a iterator and so we can use it to read one line at a time, thus reducing memory usage significantly: we never consume more memory than is needed to for one line from the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "file_name = 'files/train.csv'\n",
    "fh = {}\n",
    "col_idx = 7\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    heading = f.readline()[col_idx]  # Reads header line\n",
    "    for line in f:\n",
    "        num_pass = int(line.strip().split(',')[7])\n",
    "        fh[num_pass] = fh.get(num_pass, 0) + 1\n",
    "        \n",
    "print(heading, fh)\n",
    "print(f'Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the generator function for the second approach, with the <code>read_one_line</code> function: it uses a <code>yield</code> statement rather than <code>return</code> and so it is a generator function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_line(filename, skip_first=True):\n",
    "    with open(filename, 'r') as f:\n",
    "        if skip_first:\n",
    "            _ = f.readline()\n",
    "        for line in f:\n",
    "            yield line.strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "fh = {}\n",
    "col_idx = 7\n",
    "\n",
    "file_name = 'files/train.csv'\n",
    "read_gen = read_one_line(file_name, skip_first=False)\n",
    "heading = next(read_gen)[col_idx]\n",
    "\n",
    "for line in read_gen:\n",
    "    num_pass = int(line[col_idx])\n",
    "    fh[num_pass] = fh.get(num_pass, 0) + 1\n",
    "        \n",
    "print(heading, fh)\n",
    "print(f'Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells show an example of how you might try to speed up data input by reading multiple lines with each function call.  The code becomes a bit more complex and, in this case, does not provide much advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "    \n",
    "def read_chunks(filepath, chunksize, skip_first):\n",
    "    with open(filepath, 'r') as f:\n",
    "        if skip_first:\n",
    "            _ = f.readline()\n",
    "        while True:\n",
    "            lines = islice(f, chunksize)\n",
    "            lines = [*lines]\n",
    "            if lines:\n",
    "                yield (line.strip().split(',') for line in lines)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "file_name = 'files/train.csv'\n",
    "read_gen = read_chunks(file_name, 10000, skip_first=True)\n",
    "fh = {}\n",
    "\n",
    "for chunk in read_gen:\n",
    "    for line in chunk:\n",
    "        num_pass = int(line[7])\n",
    "        fh[num_pass] = fh.get(num_pass, 0) + 1\n",
    "        \n",
    "print(fh)\n",
    "print(f'Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>pandas</code> package also provides a method for reading data in chunks.  This requires that we combine the results from multiple <code>value_counts()</code> operations, which create frequency histograms.  In this circumstance, <code>pandas</code> does not speed up the operation, as indicated by the graph below.  In addition, using Python to read one line at a time consumes much less memory than is required for the chunksizes required by <code>pandas</code> for reasonably fast execution.\n",
    "\n",
    "While packages like <code>pandas</code> can provide some wonderful functionality, in some cases basic Python can be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "fh = pd.Series(dtype='int64')\n",
    "for chunk in pd.read_csv('files/train.csv', chunksize=2000000):\n",
    "    fh = fh.add(chunk['passenger_count'].value_counts(), fill_value=0)\n",
    "print(fh)\n",
    "print(f'Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph of <code>pandas</code> execution time versus chunksize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_time = {1000: 63.54459285736084,\n",
    " 10000: 62.875988245010376,\n",
    " 100000: 62.655011892318726,\n",
    " 1000000: 63.9449999332428,\n",
    " 2000000: 64.10800004005432,\n",
    " 5000000: 64.95298767089844,\n",
    " 10000000: 65.6650767326355}\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "x = [x for x in p_time.keys()]\n",
    "y = [y for y in p_time.values()]\n",
    "y2 = [62.5 for _ in range(len(x))]\n",
    "ax.scatter(x, y, c='b', label='pandas')\n",
    "ax.scatter(x, y2, c='gray', label='Basic Python')\n",
    "for i in range(len(p_time.keys())-1):\n",
    "    ax.plot(x[i:i+2], y[i:i+2], c='b')\n",
    "    ax.plot(x[i:i+2], y2[i:i+2], c='gray')\n",
    "ax.set_ylabel('Execution Time (sec)')\n",
    "ax.set_xlabel('Chunksize (lines)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Best Approach?:__ the best approach here is arguably to use the filestream generator directly in your code (or use a function) to access one line at a time.  It is close to the fastest approach and minimizes memory consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-pass versus Two-pass Computations <a name='1_2_pass' /> \n",
    "\n",
    "Calculations were made while streaming data in both of the preceding example rather than loading all of the data into memory at once. Finding ways to do computations while streaming data is one way to reduce memory requirements.  It is sometimes not as easy to do so for some computations.\n",
    "\n",
    "Suppose we are interested in computing the standard deviation of the number of riders.  Having already computed the frequency histogram data we could compute the standard deviation directly.  But, this is not possible if our data were floating-point and we needed to construct _bins_, in which case computing standard deviation from the frequency data would be only an approximation. \n",
    "\n",
    "This formula is typically used to compute the variance (from which we compute standard deviation) of a series of $n$ values:\n",
    "\n",
    "$ \\sum_{i=0}^{n-1}{\\left( x_i - \\bar{x} \\right)^2}$.\n",
    "\n",
    "This computation requires that we know the mean of $x$, $\\bar{x}$ prior to summing the squared differences.  So, an intuitive approach would be to do this computation in two passes, the first to compute $\\bar{x}$.   Can reasonable estiamtes be computed in one pass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = 10\n",
    "x = [i for i in range(n)]\n",
    "x_bar = sum(x) / n\n",
    "x_var = sum([(z - x_bar)**2 for z in x])/n\n",
    "print(f'Variance: {x_var: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a generator, however, presents an issue because computing the <code>sum()</code> of generator in the code cell below exhausts the generator and so no data remains in <code>x</code> to iterate over when computing <code>x_var</code>.  The result is a variance of 0, which is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x = (i for i in range(n))\n",
    "x_bar = sum(x) / n\n",
    "x_var = sum([(z - x_bar)**2 for z in x])/n\n",
    "print(f'Variance: {x_var: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator needs to be reset in order to make this second computation and so using a generator makes this a _two-pass_ computation.  This could still be a viable method given the speed and memory requirements for generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x = (i for i in range(n))\n",
    "x_bar = sum(x) / n\n",
    "\n",
    "x = (i for i in range(n))\n",
    "x_var = sum([(z - x_bar)**2 for z in x])/n\n",
    "print(f'Variance: {x_var: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to compute the variance of number of passengers in a New York City taxi in this fashion, we therefore need to instantiate the generator twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "file_name = 'files/train.csv'\n",
    "\n",
    "print('Computing mean: ', end='')\n",
    "num_pass_sum = 0\n",
    "n = 0\n",
    "read_gen = read_one_line(file_name, skip_first=True)\n",
    "for line in read_gen:\n",
    "    num_pass_sum += int(line[7])\n",
    "    n += 1\n",
    "num_pass_mean = num_pass_sum/n\n",
    "print(f'{float(time.time() - start): .2f} seconds, mean = {num_pass_mean}')\n",
    "      \n",
    "print(f'Computing variance: ', end='')\n",
    "num_pass_var = 0.0\n",
    "read_gen = read_one_line(file_name, skip_first=True)\n",
    "for line in read_gen:\n",
    "    num_pass_var += (int(line[7]) - num_pass_mean)**2\n",
    "num_pass_var = num_pass_var/n\n",
    "print(f'{float(time.time() - start): .2f} seconds; Variance = {num_pass_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to sumamrize the data in a frequency histogram and, then, compute variance from that frequency data.\n",
    "\n",
    "This approach takes only one pass through the data and then computes the variance from a condensed form of the data, which is much faster than another pass through the data.  This gives an exact result for integer data, but would be an approximation with floating-point data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "''' Compute frequency histogram '''\n",
    "start = time.time()\n",
    "file_name = 'files/train.csv'\n",
    "fh = {}\n",
    "col_idx = 7\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    heading = f.readline()[col_idx]  # Reads header line\n",
    "    for line in f:\n",
    "        num_pass = int(line.strip().split(',')[7])\n",
    "        fh[num_pass] = fh.get(num_pass, 0) + 1\n",
    "\n",
    "''' Compute variance from frequency data '''\n",
    "n = sum([v for k,v in fh.items()])\n",
    "mean = sum([k*v for k,v in fh.items()])/n\n",
    "var = sum([v/n * (k - mean)**2 for k,v in fh.items()])\n",
    "print(f'n: {n}; Mean: {mean}; Variance: {var}; Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $Var[X] = {\\bf E}[X^2] - {\\bf E}[X]^2$ and the generator in a _one-pass_ method (although there might be some numerical stability issues with this approach sometimes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "file_name = 'files/train.csv'\n",
    "\n",
    "m_x = 0\n",
    "m_x_sq = 0\n",
    "n = 0\n",
    "read_gen = read_one_line(file_name, skip_first=True)\n",
    "for line in read_gen:\n",
    "    d = int(line[7])\n",
    "    m_x += d\n",
    "    m_x_sq += d**2\n",
    "    n += 1\n",
    "mean = m_x/n\n",
    "print(f'mean = {mean}; variance = {m_x_sq/n - (mean**2)}; Execution time: {float(time.time() - start): .2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_One-pass_ approximations to variance are possible (that are numerically stable).  Here is one.  Don't worry about the details of the computation: the main point is that variance can be approximated accurately in one-pass.  In case you do want to look at the details, here is a reference:\n",
    "\n",
    "[https://planetmath.org/onepassalgorithmtocomputesamplevariance](https://planetmath.org/onepassalgorithmtocomputesamplevariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lessons:\n",
    "- Streaming big data is memory-efficient and reduces execution time\n",
    "- Making computations with streamed data requires careful consideration of how a calculation is made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
